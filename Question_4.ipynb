{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqfQT0O1tY7N",
        "outputId": "256c21c0-14c0-4e49-9bd5-3c7be766ad7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=d46d3b60ebe84221760935464951214172662f7af0fe043f13536d59bc900dac\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.16\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g1bInfSytUye"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow \n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "from math import log1p \n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91BRo-JGtPIG",
        "outputId": "b73ca037-a704-4a23-a91b-a3e140451f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0   211M      0  0:00:09  0:00:09 --:--:--  215M\n"
          ]
        }
      ],
      "source": [
        "!curl --header \"Host: storage.googleapis.com\" --header \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.128 Safari/537.36 Edg/89.0.774.77\" --header \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header \"Accept-Language: en-US,en;q=0.9\" --header \"Referer: https://github.com/google-research-datasets/dakshina\" \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\" -L -o \"dakshina_dataset_v1.0.tar\"\n",
        "shutil.unpack_archive(\"/content/dakshina_dataset_v1.0.tar\",'/content/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xRx4gCBzMpA",
        "outputId": "92dfa261-4acf-49b4-c63f-d798123b2f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  44202\n",
            "Number of validation samples:  4358\n",
            "Number of testing samples:  4502\n",
            "Number of samples: 44202\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for train inputs: 20\n",
            "Max sequence length for train outputs: 23\n",
            "Max sequence length for val inputs: 18\n",
            "Max sequence length for val outputs: 18\n",
            "Max sequence length for test inputs: 16\n",
            "Max sequence length for test outputs: 19\n",
            "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "[' ', 'E', 'S', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्', 'ॐ']\n",
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "{' ': 0, 'E': 1, 'S': 2, 'ँ': 3, 'ं': 4, 'ः': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ए': 13, 'ऐ': 14, 'ऑ': 15, 'ओ': 16, 'औ': 17, 'क': 18, 'ख': 19, 'ग': 20, 'घ': 21, 'ङ': 22, 'च': 23, 'छ': 24, 'ज': 25, 'झ': 26, 'ञ': 27, 'ट': 28, 'ठ': 29, 'ड': 30, 'ढ': 31, 'ण': 32, 'त': 33, 'थ': 34, 'द': 35, 'ध': 36, 'न': 37, 'प': 38, 'फ': 39, 'ब': 40, 'भ': 41, 'म': 42, 'य': 43, 'र': 44, 'ल': 45, 'व': 46, 'श': 47, 'ष': 48, 'स': 49, 'ह': 50, '़': 51, 'ा': 52, 'ि': 53, 'ी': 54, 'ु': 55, 'ू': 56, 'ृ': 57, 'ॅ': 58, 'े': 59, 'ै': 60, 'ॉ': 61, 'ो': 62, 'ौ': 63, '्': 64, 'ॐ': 65}\n",
            "[13.  1.  3.  8. 15. 14.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.]\n",
            "[ 2.  2. 42. 60. 23. 62.  4.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.]\n",
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "TRAIN_PATH = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "VAL_PATH = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "TEST_PATH = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n",
        "\n",
        "# Function used to load data from a given path\n",
        "def load_data(path):\n",
        "  df = pd.read_csv(path, sep='\\t', header=None)\n",
        "  df.columns = ['Devanagari', 'Romanized', 'Attestations']\n",
        "  df = df.dropna()\n",
        "  input_texts = df['Romanized'].tolist()\n",
        "  target_texts = df['Devanagari'].apply(lambda x: 'S' + x + 'E').tolist()\n",
        "  return input_texts, target_texts\n",
        "\n",
        "#loading training , testing and validation data\n",
        "train_texts, train_target_texts = load_data(TRAIN_PATH)\n",
        "val_texts, val_target_texts = load_data(VAL_PATH)\n",
        "test_texts, test_target_texts = load_data(TEST_PATH)\n",
        "\n",
        "print(\"Number of training samples: \", len(train_texts))\n",
        "print(\"Number of validation samples: \", len(val_texts))\n",
        "print(\"Number of testing samples: \", len(test_texts))\n",
        "\n",
        "train_indices = np.arange(len(train_texts))\n",
        "val_indices = np.arange(len(val_texts))\n",
        "test_indices = np.arange(len(test_texts))\n",
        "\n",
        "np.random.shuffle(train_indices)\n",
        "np.random.shuffle(val_indices)\n",
        "\n",
        "# Used to store vocabulary of source and target language\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "# Used to store texts after adding start and end token\n",
        "train_target_texts_processed = []\n",
        "val_target_texts_processed = []\n",
        "test_target_texts_processed = []\n",
        "\n",
        "# Adding starting and ending token in training data\n",
        "for (input_text, target_text) in zip(train_texts, train_target_texts):\n",
        "    # \"S\" -> start token, \"E\" -> end token, \" \" -> pad token\n",
        "    target_text = \"S\" + target_text + \"E\"\n",
        "    train_target_texts_processed.append(target_text)\n",
        "    for char in input_text:\n",
        "      input_characters.add(char)\n",
        "    for char in target_text:\n",
        "      target_characters.add(char)\n",
        "\n",
        "# Adding starting and ending token in validation data\n",
        "for (input_text, target_text) in zip(val_texts, val_target_texts):\n",
        "    # \"S\" -> start token, \"E\" -> end token, \" \" -> pad token\n",
        "    target_text = \"S\" + target_text + \"E\"\n",
        "    val_target_texts_processed.append(target_text)\n",
        "    for char in input_text:\n",
        "      input_characters.add(char)\n",
        "    for char in target_text:\n",
        "      target_characters.add(char)\n",
        "\n",
        "# Adding starting and ending token in testing data\n",
        "for (input_text, target_text) in zip(test_texts, test_target_texts):\n",
        "    # \"S\" -> start token, \"E\" -> end token, \" \" -> pad token\n",
        "    target_text = \"S\" + target_text + \"E\"\n",
        "    test_target_texts_processed.append(target_text)\n",
        "    for char in input_text:\n",
        "      input_characters.add(char)\n",
        "    for char in target_text:\n",
        "      target_characters.add(char)\n",
        "\n",
        "input_texts = list(map(train_texts.__getitem__, train_indices))\n",
        "target_texts = list(map(train_target_texts_processed.__getitem__, train_indices))\n",
        "\n",
        "val_input_texts = list(map(val_texts.__getitem__, val_indices))\n",
        "val_target_texts = list(map(val_target_texts_processed.__getitem__, val_indices))\n",
        "\n",
        "test_input_texts = list(map(test_texts.__getitem__, test_indices))\n",
        "test_target_texts = list(map(test_target_texts_processed.__getitem__, test_indices))\n",
        "\n",
        "# Creating sorted vocabulary of source and target language\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "# Add pad tokens\n",
        "input_characters.insert(0, \" \")\n",
        "target_characters.insert(0, \" \")\n",
        "\n",
        "# Creating essential parameters\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(t) for t in input_texts])\n",
        "max_decoder_seq_length = max([len(t) for t in target_texts])\n",
        "val_max_encoder_seq_length = max([len(t) for t in val_input_texts])\n",
        "val_max_decoder_seq_length = max([len(t) for t in val_target_texts])\n",
        "\n",
        "test_max_encoder_seq_length = max([len(t) for t in test_input_texts])\n",
        "test_max_decoder_seq_length = max([len(t) for t in test_target_texts])\n",
        "\n",
        "# Mapping each character of vocabulary to index\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# defining shapes of input sequence of encoder after padding for training data\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "# defining shapes of input and target sequence of decoder after padding for training data\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "# Adding training data\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    encoder_input_data[i, t+1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t-1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "# defining shapes of input sequence of encoder after padding for validation data\n",
        "val_encoder_input_data = np.zeros((len(input_texts), val_max_encoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "# defining shapes of input and target sequence of decoder after padding for validation data\n",
        "val_decoder_input_data = np.zeros((len(input_texts), val_max_decoder_seq_length), dtype=\"float32\")\n",
        "val_decoder_target_data = np.zeros((len(input_texts), val_max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "# Adding validation data\n",
        "for i, (input_text, target_text) in enumerate(zip(val_input_texts, val_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        val_encoder_input_data[i, t] = input_token_index[char]\n",
        "    val_encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        val_decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            val_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    val_decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    val_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "# defining shapes of input sequence of encoder after padding for testing data\n",
        "test_encoder_input_data = np.zeros((len(input_texts), test_max_encoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "# defining shapes of input and target sequence of decoder after padding for testing data\n",
        "test_decoder_input_data = np.zeros((len(input_texts), test_max_decoder_seq_length), dtype=\"float32\")\n",
        "test_decoder_target_data = np.zeros((len(input_texts), test_max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "# Adding testing data\n",
        "for i, (input_text, target_text) in enumerate(zip(test_input_texts, test_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        test_encoder_input_data[i, t] = input_token_index[char]\n",
        "    test_encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        test_decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            test_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    test_decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    test_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "# creating inverse map which maps integer to character\n",
        "inverse_input_token_index = dict((i, char) for char, i in input_token_index.items())\n",
        "inverse_target_token_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for train inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for train outputs:\", max_decoder_seq_length)\n",
        "print(\"Max sequence length for val inputs:\", val_max_encoder_seq_length)\n",
        "print(\"Max sequence length for val outputs:\", val_max_decoder_seq_length)\n",
        "print(\"Max sequence length for test inputs:\", test_max_encoder_seq_length)\n",
        "print(\"Max sequence length for test outputs:\", test_max_decoder_seq_length)\n",
        "print(input_characters)\n",
        "print(target_characters)\n",
        "print(input_token_index)\n",
        "print(target_token_index)\n",
        "print(encoder_input_data[10])\n",
        "print(decoder_input_data[10])\n",
        "print(decoder_target_data[10])\n",
        "\n",
        "\n",
        "class TransliterationModel(object):\n",
        "  def __init__(self, config):\n",
        "    self.config = config\n",
        "\n",
        "  def train_and_evaluate(self, encoder_input_data, decoder_input_data, decoder_target_data,\n",
        "                         val_encoder_input_data, val_target_texts, test_encoder_input_data, test_target_texts):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None, ),name = 'Encoder_inputs')\n",
        "\n",
        "    # Embedding layer: (num_encoder_tokens, input_embedding_size)\n",
        "    encoder_embedded =  Embedding(num_encoder_tokens, self.config.input_embedding_size,\n",
        "                         mask_zero=True, name='Encoder_embeddings')(encoder_inputs)\n",
        "    encoder_outputs = encoder_embedded\n",
        "\n",
        "    # Adding encoder layers and storing encoder states according to cell type\n",
        "    if self.config.cell_type == 'RNN':\n",
        "      encoder_layers = [SimpleRNN(self.config.hidden_units, \n",
        "                                  dropout=self.config.dropout, \n",
        "                                  return_sequences=True, \n",
        "                                  return_state=True, \n",
        "                                  name=f\"Encoder_{layer_idx}\")\n",
        "                        for layer_idx in range(self.config.num_encoder_layers)]\n",
        "      encoder_outputs, hidden = encoder_layers[0](encoder_outputs)\n",
        "      encoder_states = [hidden]\n",
        "      for layer_idx in range(1, self.config.num_encoder_layers):\n",
        "        encoder_outputs, hidden = encoder_layers[layer_idx](encoder_outputs, initial_state=encoder_states)\n",
        "        encoder_states = [hidden]  \n",
        "    elif self.config.cell_type == 'LSTM':\n",
        "      encoder_layers = [LSTM(self.config.hidden_units, \n",
        "                             dropout=self.config.dropout, \n",
        "                             return_sequences=True, \n",
        "                             return_state=True, \n",
        "                             name=f\"Encoder_{layer_idx}\")\n",
        "                        for layer_idx in range(self.config.num_encoder_layers)]\n",
        "      encoder_outputs, hidden, context = encoder_layers[0](encoder_outputs)\n",
        "      encoder_states = [hidden, context]\n",
        "      for layer_idx in range(1, self.config.num_encoder_layers):\n",
        "        encoder_outputs, hidden, context = encoder_layers[layer_idx](encoder_outputs, initial_state=encoder_states)\n",
        "        encoder_states = [hidden, context]\n",
        "    elif self.config.cell_type == 'GRU':\n",
        "      encoder_layers = [GRU(self.config.hidden_units, \n",
        "                            dropout=self.config.dropout, \n",
        "                            return_sequences=True, \n",
        "                            return_state=True, \n",
        "                            name=f\"Encoder_{layer_idx}\")\n",
        "                        for layer_idx in range(self.config.num_encoder_layers)]\n",
        "      encoder_outputs, hidden = encoder_layers[0](encoder_outputs)\n",
        "      encoder_states = [hidden]\n",
        "      for layer_idx in range(1, self.config.num_encoder_layers):\n",
        "        encoder_outputs, hidden = encoder_layers[layer_idx](encoder_outputs, initial_state=encoder_states)\n",
        "        encoder_states = [hidden]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,), name = 'Decoder_inputs')\n",
        "\n",
        "    # Embedding layer: (num_decoder_tokens, hidden_units)\n",
        "    decoder_embedded = Embedding(num_decoder_tokens, self.config.hidden_units,\n",
        "                       mask_zero=True, name='Decoder_embeddings')(decoder_inputs)\n",
        "    decoder_outputs = decoder_embedded\n",
        "\n",
        "    # Adding decoder layers and storing decoder states according to cell type\n",
        "    if self.config.cell_type == 'RNN':\n",
        "      decoder_layers = [SimpleRNN(self.config.hidden_units, \n",
        "                                  dropout=self.config.dropout, \n",
        "                                  return_sequences=True, \n",
        "                                  return_state=True, \n",
        "                                  name=f\"Decoder_{layer_idx}\")\n",
        "                        for layer_idx in range(self.config.num_decoder_layers)]\n",
        "      decoder_outputs, _ = decoder_layers[0](decoder_outputs, initial_state=encoder_states)\n",
        "      for layer_idx in range(1, self.config.num_decoder_layers):\n",
        "        decoder_outputs, _ = decoder_layers[layer_idx](decoder_outputs, initial_state = encoder_states)\n",
        "    if self.config.cell_type == 'LSTM':\n",
        "      decoder_layers = [LSTM(self.config.hidden_units, \n",
        "                             dropout=self.config.dropout, \n",
        "                             return_sequences=True, \n",
        "                             return_state=True, \n",
        "                             name=f\"Decoder_{layer_idx}\")\n",
        "                        for layer_idx in range(self.config.num_decoder_layers)]\n",
        "      decoder_outputs, _, _ = decoder_layers[0](decoder_outputs, initial_state=encoder_states)\n",
        "      for layer_idx in range(1, self.config.num_decoder_layers):\n",
        "        decoder_outputs, _, _ = decoder_layers[layer_idx](decoder_outputs, initial_state = encoder_states)\n",
        "    elif self.config.cell_type == 'GRU':\n",
        "      decoder_layers = [GRU(self.config.hidden_units, \n",
        "                            dropout=self.config.dropout, \n",
        "                            return_sequences=True, \n",
        "                            return_state=True, \n",
        "                            name=f\"Decoder_{layer_idx}\")\n",
        "                        for layer_idx in range(self.config.num_decoder_layers)]\n",
        "      decoder_outputs, _ = decoder_layers[0](decoder_outputs, initial_state=encoder_states)\n",
        "      for layer_idx in range(1, self.config.num_decoder_layers):\n",
        "        decoder_outputs, _ = decoder_layers[layer_idx](decoder_outputs, initial_state=encoder_states)\n",
        "    decoder_outputs = Dense(num_decoder_tokens, activation='softmax', name='dense')(decoder_outputs)\n",
        "\n",
        "    # Defining our Seq2seq model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    optimizer = Adam(learning_rate=self.config.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=self.config.batch_size,\n",
        "        epochs=self.config.epochs,\n",
        "        validation_data=([val_encoder_input_data, val_decoder_input_data], val_decoder_target_data)\n",
        "    )\n",
        "\n",
        "    \n",
        "    # Wrap Encoder Decoder\n",
        "    encoder_inputs = model.input[0]\n",
        "    if self.config.cell_type in ['RNN', 'GRU']:\n",
        "      encoder_outputs, hidden_state = model.get_layer(f'Encoder_{self.config.num_encoder_layers-1}').output\n",
        "      encoder_states = [hidden_state]\n",
        "      encoder = Model(encoder_inputs, encoder_states)\n",
        "      decoder_inputs = model.input[1]\n",
        "      decoder_outputs = model.get_layer('Decoder_embeddings')(decoder_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "      for i in range(self.config.num_decoder_layers):\n",
        "        decoder_hidden = keras.Input(shape=(self.config.hidden_units,))\n",
        "        states = [decoder_hidden]\n",
        "        decoder_outputs, hidden_state_decoder = model.get_layer(f'Decoder_{i}')(decoder_outputs, initial_state=states)\n",
        "        decoder_states += [hidden_state_decoder]\n",
        "        decoder_states_inputs += states\n",
        "    elif self.config.cell_type == 'LSTM':\n",
        "      encoder_outputs, hidden_state, context_state = model.get_layer(f'Encoder_{self.config.num_encoder_layers-1}').output\n",
        "      encoder_states = [hidden_state, context_state]\n",
        "      encoder = Model(encoder_inputs, encoder_states)\n",
        "      decoder_inputs = model.input[1]  # input_1\n",
        "      decoder_outputs = model.get_layer('Decoder_embeddings')(decoder_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "      for i in range(self.config.num_decoder_layers):\n",
        "        decoder_hidden = keras.Input(shape=(self.config.hidden_units,))\n",
        "        decoder_context = keras.Input(shape=(self.config.hidden_units,))\n",
        "        states = [decoder_hidden, decoder_context]\n",
        "        decoder = model.get_layer(f'Decoder_{i}')\n",
        "        decoder_outputs, hidden_state_decoder, context_state_decoder = decoder(decoder_outputs, initial_state=states)\n",
        "        decoder_states += [hidden_state_decoder, context_state_decoder]\n",
        "        decoder_states_inputs += states\n",
        "    decoder_dense = model.get_layer('dense')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        " \n",
        "    #calculating test accuracy\n",
        "    total, correct = 0, 0\n",
        "    input_list,output_list, target_list = [], [], []\n",
        "    for i in range(len(test_texts)):\n",
        "      curr_input =  test_input_texts[i]\n",
        "      input_list.append(curr_input.replace(' ', '').replace('S', '').replace('E', ''))\n",
        "      output = self.decode_to_text(test_encoder_input_data[i:i+1], encoder, decoder)\n",
        "      target = test_target_texts[i][1:len(test_target_texts[i])-1]\n",
        "      output = output[0:len(output)-1]\n",
        "      output = output.replace(' ', '').replace('S', '').replace('E', '')\n",
        "      target = target.replace(' ', '').replace('S', '').replace('E', '')\n",
        "      output_list.append(output)\n",
        "      target_list.append(target)\n",
        "      \n",
        "      if output == target:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "      test_word_accuracy = correct / total\n",
        "      \n",
        "    word_test_accuracy = correct / total\n",
        "    print(\"Test Accuracy: \",word_test_accuracy )\n",
        "\n",
        "    # Making predictions_vanilla.csv\n",
        "    df = pd.DataFrame({'inputs': input_list,'targets': target_list,'predictions': output_list})\n",
        "    df.to_csv('predictions_vanilla.csv')\n",
        "    \n",
        "    \n",
        "  def decode_to_text(self, inputs, encoder, decoder):\n",
        "    encoder_states = [encoder.predict(inputs) for _ in range(self.config.num_decoder_layers)]\n",
        "    target = np.array([[target_token_index['S']]])\n",
        "    sentence, done = \"\", False\n",
        "    beam_width = 1 if self.config.decoding_strategy == 'greedy' else self.config.beam_width\n",
        "    sentence = self.beam_search_decoder(inputs, encoder, decoder, beam_width)\n",
        "    return sentence\n",
        "  \n",
        "\n",
        "  def beam_search_decoder(self, inputs, encoder, decoder, beam_width):\n",
        "        \n",
        "        done, decoded_sentence = False, \"\"\n",
        "\n",
        "\n",
        "        # Get encoder states\n",
        "        encoder_states = [encoder.predict(inputs) for _ in range(self.config.num_decoder_layers)]\n",
        "\n",
        "        # Decoder input begins with Start Token \"S\"\n",
        "        target_sequence = np.array([[target_token_index[\"S\"]]])\n",
        "\n",
        "        # sum_of_log_probs (score), flag for end of current sequence, target_sequence, states , sequence_token, sequence_char\n",
        "        sequences = [[0.0, 0,  target_sequence, encoder_states,  list(),list()]]\n",
        "        while not done:\n",
        "            candidates = list()\n",
        "            for i in range(len(sequences)):\n",
        "              output = decoder.predict([sequences[i][2]] + sequences[i][3])\n",
        "              output_tokens, states = output[0], output[1:]\n",
        "              prob = output_tokens[0,-1,:]\n",
        "              \n",
        "              score, flag, _, _, sequence_token, sequence_char = sequences[i]\n",
        "              \n",
        "              if flag == 0:\n",
        "                for j in range(len(inverse_target_token_index)):\n",
        "                  char = inverse_target_token_index[j]\n",
        "                  target_sequence = np.array([[j]])\n",
        "                  candidate = [score - np.log(prob[j]), 0, target_sequence, states,  sequence_token + [j] , sequence_char + [char] ]\n",
        "                  candidates.append(candidate)\n",
        "            sorted_candidates = sorted(candidates, key=lambda x:x[0])\n",
        "            k = min(beam_width, len(sorted_candidates))\n",
        "            sequences = sorted_candidates[:k]\n",
        "            done = True\n",
        "           \n",
        "            for sequence in range(len(sequences)):\n",
        "                score, flag, tgt_seq, states, sequence_token, sequence_char = sequences[sequence]\n",
        "                if (len(sequence_char) > max_decoder_seq_length) or (sequence_char[-1] == \"E\"): \n",
        "                  flag = 1\n",
        "                sequences[sequence][1] = flag\n",
        "                done = False if flag == 0 else done\n",
        "            if sequences[0][-1][-1]==\"E\": \n",
        "              done = True\n",
        "        top_decoded_sentence = ''.join(sequences[0][5])\n",
        "        return top_decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM_em8aetuJT",
        "outputId": "cbe77f58-2683-40aa-becd-66c28e31cd42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "173/173 [==============================] - 57s 207ms/step - loss: 1.0293 - accuracy: 0.4242 - val_loss: 0.1032 - val_accuracy: 0.4852\n",
            "Epoch 2/15\n",
            "173/173 [==============================] - 29s 167ms/step - loss: 0.7812 - accuracy: 0.5215 - val_loss: 0.0814 - val_accuracy: 0.5856\n",
            "Epoch 3/15\n",
            "173/173 [==============================] - 29s 168ms/step - loss: 0.5654 - accuracy: 0.6398 - val_loss: 0.0535 - val_accuracy: 0.7115\n",
            "Epoch 4/15\n",
            "173/173 [==============================] - 29s 168ms/step - loss: 0.3805 - accuracy: 0.7444 - val_loss: 0.0362 - val_accuracy: 0.8014\n",
            "Epoch 5/15\n",
            "173/173 [==============================] - 29s 167ms/step - loss: 0.2768 - accuracy: 0.8087 - val_loss: 0.0280 - val_accuracy: 0.8415\n",
            "Epoch 6/15\n",
            "173/173 [==============================] - 29s 167ms/step - loss: 0.2203 - accuracy: 0.8461 - val_loss: 0.0245 - val_accuracy: 0.8591\n",
            "Epoch 7/15\n",
            "173/173 [==============================] - 29s 168ms/step - loss: 0.1850 - accuracy: 0.8706 - val_loss: 0.0217 - val_accuracy: 0.8746\n",
            "Epoch 8/15\n",
            "173/173 [==============================] - 29s 167ms/step - loss: 0.1610 - accuracy: 0.8868 - val_loss: 0.0201 - val_accuracy: 0.8840\n",
            "Epoch 9/15\n",
            "173/173 [==============================] - 29s 168ms/step - loss: 0.1429 - accuracy: 0.8998 - val_loss: 0.0189 - val_accuracy: 0.8911\n",
            "Epoch 10/15\n",
            "173/173 [==============================] - 29s 167ms/step - loss: 0.1282 - accuracy: 0.9096 - val_loss: 0.0182 - val_accuracy: 0.8953\n",
            "Epoch 11/15\n",
            "173/173 [==============================] - 29s 168ms/step - loss: 0.1159 - accuracy: 0.9182 - val_loss: 0.0179 - val_accuracy: 0.8981\n",
            "Epoch 12/15\n",
            "173/173 [==============================] - 29s 168ms/step - loss: 0.1056 - accuracy: 0.9252 - val_loss: 0.0176 - val_accuracy: 0.8986\n",
            "Epoch 13/15\n",
            "173/173 [==============================] - 29s 167ms/step - loss: 0.0966 - accuracy: 0.9317 - val_loss: 0.0173 - val_accuracy: 0.9015\n",
            "Epoch 14/15\n",
            "173/173 [==============================] - 29s 168ms/step - loss: 0.0882 - accuracy: 0.9375 - val_loss: 0.0171 - val_accuracy: 0.9031\n",
            "Epoch 15/15\n",
            "173/173 [==============================] - 29s 168ms/step - loss: 0.0807 - accuracy: 0.9430 - val_loss: 0.0171 - val_accuracy: 0.9030\n",
            "Test Accuracy:  0.38360728565082186\n"
          ]
        }
      ],
      "source": [
        "# Best hyperparameter configuration\n",
        "cfg_dict = {'batch_size': 256, 'beam_width': 5, 'cell_type': 'GRU', 'decoding_strategy': 'greedy', 'dropout': 0.3, 'epochs': 15, 'hidden_units': 512, 'input_embedding_size': 256, 'learning_rate': 0.0005, 'num_decoder_layers': 3, 'num_encoder_layers': 1}\n",
        "\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "cfg = dotdict(cfg_dict)\n",
        "model_transliteration = TransliterationModel(cfg)\n",
        "model_transliteration.train_and_evaluate(encoder_input_data,decoder_input_data,decoder_target_data,val_encoder_input_data, val_target_texts, test_encoder_input_data, test_target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmYHRqVIuGZ7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Question_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}